# -*- coding: utf-8 -*-
"""assessment

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IYT7ix910S-2SiKhTUrp_9S80tM_LfaM
"""

#importing all required libraries
from bs4 import BeautifulSoup
import requests
import pandas as pd

df = pd.read_excel('Input.xlsx')    #reading input file

title_name = []
text_content = []
for i in df['URL']:
  html_text = requests.get(f'{i}').text
  soup = BeautifulSoup(html_text,'lxml')
  title = soup.find('h1',attrs={'class','entry-title','tdb-title-text'})            #accessing the title
  content=soup.findAll('div', attrs={'class','td-post-content','tbd-block-inner'})  # accessing the content
  if title == None:
    title_name.append('Data Not Available')
    text_content.append('Data Not Available')
  else:
    title_name.append(title.text)
    content=content[0].text.replace('\n'," ")
    text_content.append(content)



df = pd.DataFrame({'title_name':title_name,'content':text_content})    # making dataframe



df.head()

df.shape

# converting the text into lowercase
def con_to_lower(text):
  return text.lower()

df['content'] =  df['content'].apply(con_to_lower)

# removing punctuation
import string
def remove_punc(text):
  return text.translate(str.maketrans('', '',string.punctuation))

df['content'] =  df['content'].apply(remove_punc)

df['content'][9]

import re            #using regular expression
def clean_text(text):
  text = re.sub('[”“–’‘]','',text)
  text = re.sub('\xa0','',text)
  return text

df['content'] = df['content'].apply(clean_text)

#tokenization
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
def tokenization(text):
  return word_tokenize(text)

df['content'] = df['content'].apply(tokenization)



# removal of stopwords
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = stopwords.words('english')
def remove_stopwords(text):
  return [word for word in text if not word in stop_words]

df['content']=df['content'].apply(remove_stopwords)



#calculating the positive score
positive_score = []
for i in df['content']:
  with open("positive-words.txt","r") as pos:
    posi_words = pos.read().split("\n")
  pos_count = " ".join ([w for w in i if w in posi_words])
  pos_count=pos_count.split(" ")
  Positive_score=len(pos_count)
  positive_score.append(Positive_score)





# calculating the negative score
negative_score = []
for i in df['content']:
  with open("negative-words.txt","r",encoding = "ISO-8859-1") as neg:
    negwords = neg.read().split("\n")
  neg_count = " ".join ([w for w in i if w in negwords])
  neg_count=neg_count.split(" ")
  Negative_score=len(neg_count)
  negative_score.append(Negative_score)



# for calculating polarity and Subjectivity
Polarity = []
Subjectivity = []
from textblob import TextBlob
for i in text_content:
    sentiment = TextBlob(i).sentiment

    Polarity.append(sentiment.polarity)
    Subjectivity.append(sentiment.subjectivity)



#calculating the average sentence length
average_length = []
for i in text_content:
  avg_len = len(i.replace(' ',''))/len(re.split(r'[?!.]', i))
  average_length.append(avg_len)



#calculating the percentage of complex words
PER_COMPLEX_WORDS = []
for word in text_content:
    count = 0
    vowels = "AEIOUYaeiouy"
    if word[0] in vowels:
        count += 1
    for index in range(1, len(word)):
        if word[index] in vowels and word[index - 1] not in vowels:
            count += 1
            if word.endswith("es"or "ed"):
                count -= 1
    if count == 0:
        count += 1
    PER_COMPLEX_WORDS.append((count*100)/len(word))



pip install textstat

# calculating fog index
import textstat
fog_index = []
for i in text_content:
  FOG_INDEX=(textstat.gunning_fog(i))
  fog_index.append(FOG_INDEX)



# average number of words per sentence
avg_num_of_words = []
for i in text_content:
  n = len(i.replace(' ',''))/len(i.split(' '))
  avg_num_of_words.append(n)

#complex word count
complex_word_count = []
for word in text_content:
    count = 0
    vowels = "AEIOUYaeiouy"
    if word[0] in vowels:
        count += 1
    for index in range(1, len(word)):
        if word[index] in vowels and word[index - 1] not in vowels:
            count += 1
            if word.endswith("es"or "ed"):
                count -= 1
    if count == 0:
        count += 1
    complex_word_count.append(count)



#word count
word_count = []
for i in text_content:
  word_count.append(len(i.split(' ')))



#syllable per word
syll = []
for i in text_content:
  word=i.replace(' ','')
  syllable_count=0
  for w in word:
    if(w=='a' or w=='e' or w=='i' or w=='o' or w=='y' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U' or w=='Y'):
      syllable_count=syllable_count+1
  syll.append(syllable_count/len(i.split()))



#personal pronouns
personal_pronouns = []
for text in text_content:
    count = 0
    sentences = nltk.sent_tokenize(text)
    for sentence in sentences:
        words = nltk.word_tokenize(sentence)
        tagged = nltk.pos_tag(words)
        for (word, tag) in tagged:
            if tag == 'PRP': # If the word is a proper noun
                count = count + 1

    personal_pronouns.append(count)



#average word length
avg_word_len = []
for i in text_content:
  n = len(i.replace(' ',''))/len(i.split(' '))
  avg_word_len.append(n)



output = pd.read_excel('Output Data Structure.xlsx')

output.head(0)

output['POSITIVE SCORE'] = positive_score
output['NEGATIVE SCORE'] = negative_score
output['POLARITY SCORE'] = Polarity
output['SUBJECTIVITY SCORE'] = Subjectivity
output['AVG SENTENCE LENGTH'] = average_length
output['PERCENTAGE OF COMPLEX WORDS'] = PER_COMPLEX_WORDS
output['FOG INDEX'] = fog_index
output['AVG NUMBER OF WORDS PER SENTENCE']  = avg_num_of_words
output['COMPLEX WORD COUNT'] = complex_word_count
output['WORD COUNT'] = word_count
output['SYLLABLE PER WORD'] = syll
output['PERSONAL PRONOUNS'] = personal_pronouns
output['AVG WORD LENGTH'] = avg_word_len



# some websites in the input.xlsx file  having error
p = 0
for i in text_content:
  if i == 'Data Not Available':
    print(p)           # finding the index of links having error
    p+=1
  else:
    p+=1

output.loc[[24,37],'POSITIVE SCORE':] = 'Data Not Available'

output.head()

output.to_csv('final_output.csv')    #downloading the final output





